{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcfb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. res\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\")\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    demo = demo_list[0] or {}\n",
    "    demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "    demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "    demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "    demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "    if not demo_q or not demo_sparql:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    demo_answer = (\n",
    "        \"<Answer>\\n\"\n",
    "        f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "    )\n",
    "\n",
    "    content = (\n",
    "        f\"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        f\"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        f'with key \\\"sparql\\\" and the query as its string value.\\n\\n'\n",
    "        f\"Example INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "        f\"Question:\\n{demo_q}\\n\\n\"\n",
    "        f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "        f\"Example OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "        f\"{demo_answer}\\n\"\n",
    "    )\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-2Q5PvWRvbSkk31o3FuDUGv\n",
      "Batch ID: batch_6897d10779448190bd9289e32bedc883\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    # If you want a dict form:\n",
    "    # print(batch.model_dump())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3df942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_dynamic_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9e122",
   "metadata": {},
   "source": [
    "3 Demo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any):\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\",)\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "        demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    # 3) Convert to OpenAI Batch JSONL\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a9b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-9wq7rEm1m2W88oTx947wTF\n",
      "Batch ID: batch_6898c9c9668c81908b285145e18863d4\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Download results\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_with_3_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81bc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_with_3_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_3_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d9123",
   "metadata": {},
   "source": [
    "5 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c16592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\",)\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "        demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    # 3) Convert to OpenAI Batch JSONL\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b9bfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-EU9dLAi742qYDF57iAAdTw\n",
      "Batch ID: batch_6898dd2d8d808190ac0cd213d56093dc\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Download results\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_with_5_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5703e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_with_5_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_with_5_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d6e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 1000 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writers \n",
      "[2/2] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\")\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses the first dynamic pair (if present) as a worked example.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    demo = demo_list[0] or {}\n",
    "    demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "    demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "    demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "    demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "    if not demo_q or not demo_sparql:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    demo_answer = (\n",
    "        \"<Answer>\\n\"\n",
    "        f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "    )\n",
    "\n",
    "    content = (\n",
    "        f\"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        f\"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        f'with key \\\"sparql\\\" and the query as its string value.\\n\\n'\n",
    "        f\"Example INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "        f\"Question:\\n{demo_q}\\n\\n\"\n",
    "        f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "        f\"Example OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "        f\"{demo_answer}\\n\"\n",
    "    )\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573951e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-Wyu9HdoXoCSHw3bHTrvdY7\n",
      "Batch ID: batch_689d02bc6dc88190af1251ca01785e00\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch failed! Full batch object:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LCQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    # If you want a dict form:\n",
    "    # print(batch.model_dump())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# 4️⃣ Download results\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_with_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6985b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_1_dynamic_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_1_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c304a2",
   "metadata": {},
   "source": [
    "LCQUAD - 3 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb0db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 1000 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_3_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writer\n",
      "[2/2] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_3_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_3_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_3_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\",)\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "        demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bb1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-Xcu3SfGhiZ9G29orxwny5W\n",
      "Batch ID: batch_689d05c1528c819096ae37e6b4aa5737\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_3_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LCQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_with_3_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "047cf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_3_dynamic_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_with_3_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_3_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6028b26",
   "metadata": {},
   "source": [
    "DY-Few-Shot-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d12f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 1000 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_5_dynamic_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writer\n",
      "[2/2] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_5_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_5_dynamic_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_5_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\",)\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_triples = demo.get(\"retrieved_triples_top10\", [])[:triples_limit]\n",
    "        demo_triples_str = lists_to_numbered_string(demo_triples) if demo_triples else \"(none)\"\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4699f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-FF5hCiBtoM4uJ3sQ7HFFfy\n",
      "Batch ID: batch_689d0b0b8a208190af6f54e8f2b5651d\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_5_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LCQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_5_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a0678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_5_dynamic_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_5_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/lcquad_test_solo_stage_10_with_5_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9faf2",
   "metadata": {},
   "source": [
    "QALD-10-Zero-Shot-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e5ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 359 inference records to /home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_inference.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. Given the user question, output only valid JSON in this format:\\n\\n{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nAfter whom is the Riemannian geometry named?\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[2/2] Wrote 359 batch lines to /home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.json\"\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_inference.jsonl\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. \"\n",
    "    \"Given the user question, output only valid JSON in this format:\\n\\n\"\n",
    "    \"{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        user_msg = {\"role\": \"user\", \"content\": f\"Question:\\n{question}\"}\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fde308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-78wfRnXiayQUn1Hd8Y3WA5\n",
      "Batch ID: batch_68a5e39851ac8190976bf0510e33c21a\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD-10 Zero-Shot test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    # If you want a dict form:\n",
    "    # print(batch.model_dump())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f947b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_plus_gold.json. Total records: 359\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_plus_gold.json\")\n",
    "\n",
    "ANSWER_PREFIX_RE = re.compile(r'<Answer>\\s*', re.DOTALL)\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        first_newline = s.find(\"\\n\")\n",
    "        if first_newline != -1:\n",
    "            s = s[first_newline+1:]\n",
    "        if s.endswith(\"```\"):\n",
    "            s = s[:-3]\n",
    "    return s.strip()\n",
    "\n",
    "def _first_json_object(s: str):\n",
    "    dec = json.JSONDecoder()\n",
    "    i = 0\n",
    "    while True:\n",
    "        start = s.find(\"{\", i)\n",
    "        if start == -1:\n",
    "            return None\n",
    "        try:\n",
    "            obj, end = dec.raw_decode(s[start:])\n",
    "            return obj\n",
    "        except json.JSONDecodeError:\n",
    "            i = start + 1\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    m = ANSWER_PREFIX_RE.search(content)\n",
    "    if m:\n",
    "        content = content[m.end():]\n",
    "\n",
    "    content = _strip_code_fences(content)\n",
    "\n",
    "    # If it's already a pure JSON object, parse directly\n",
    "    s = content.strip()\n",
    "    if s.startswith(\"{\"):\n",
    "        try:\n",
    "            data = json.loads(s)\n",
    "            return data.get(\"sparql\", \"\") if isinstance(data, dict) else \"\"\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # fall through to scanning\n",
    "\n",
    "    # Otherwise, scan the string to find the first JSON object and parse it\n",
    "    data = _first_json_object(content)\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"sparql\", \"\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        cid = rec.get(\"custom_id\")\n",
    "        resp = rec.get(\"response\") or {}\n",
    "        status = resp.get(\"status_code\")\n",
    "        err = rec.get(\"error\")\n",
    "\n",
    "        # Skip errored/unsuccessful results\n",
    "        if err is not None or status != 200:\n",
    "            pred_lookup[cid] = \"\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            content = resp[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pred_lookup[cid] = \"\"\n",
    "            continue\n",
    "\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca98d47",
   "metadata": {},
   "source": [
    "QALD-10-Zero-Shot-Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d86634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 356 training records to /home/m2khoda/dual_retriever/datasets/qald_10/qald_10_train_zero_shot_finetuning.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. Given the user question, output only valid JSON in this format:\\n\\n{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nWhich artists were born on the same date as Rachel Stevens?\\n\\nCandidate Triples (max 10, numbered):\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"<Answer>\\n{\\\"sparql\\\": \\\"SELECT ?s WHERE { wd:Q241665 wdt:P569 ?date . ?s wdt:P106 / wdt:P279* wd:Q483501 . ?s wdt:P569 ?date . }\\\"}\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ─── Configuration ──────────────────────────────────────────────────────────\n",
    "triples_limit = 0\n",
    "input_path  = '/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_train_retrieved_triples_filtered.json'\n",
    "output_path = '/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_train_zero_shot_finetuning.jsonl'\n",
    "\n",
    "# ─── Helper: numbered triple list ───────────────────────────────────────────\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. \"\n",
    "    \"Given the user question, output only valid JSON in this format:\\n\\n\"\n",
    "    \"{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
    ")\n",
    "\n",
    "system_msg_generic   = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "# ─── Format SPARQL into standard <Answer> + JSON string ────────────────────\n",
    "def sparql_formatter(raw_query: str) -> str:\n",
    "    one_line = ' '.join(raw_query.strip().split())\n",
    "    return \"<Answer>\\n\" + json.dumps({\"sparql\": one_line}, ensure_ascii=False)\n",
    "\n",
    "# ─── Build training dataset ────────────────────────────────────────────────\n",
    "new_dataset = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Only start from the 3rd record (index 2)\n",
    "for idx, sample in enumerate(dataset[2:], start=2):\n",
    "    question = sample['question']\n",
    "    raw_hits = sample.get('retrived_triples_ranked', [])[:triples_limit]\n",
    "    triples = [hit['triple'] for hit in raw_hits]\n",
    "    triples_str = lists_to_numbered_string(triples)\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "    }\n",
    "\n",
    "    gold_query = sample['formated_query']\n",
    "    assistant_msg = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": sparql_formatter(gold_query)\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_generic\n",
    "\n",
    "    new_dataset.append({\n",
    "        \"messages\": [system_msg, user_msg, assistant_msg]\n",
    "    })\n",
    "\n",
    "# ─── Write JSONL ────────────────────────────────────────────────────────────\n",
    "with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "    for record in new_dataset:\n",
    "        f_out.write(json.dumps(record) + '\\n')\n",
    "\n",
    "# ─── Sanity Check ──────────────────────────────────────────────────────────\n",
    "print(f\"Wrote {len(new_dataset)} training records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(new_dataset[0], indent=2)[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169bc866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-PvDgBKossQWNnL5Z3P22ty', bytes=188308, created_at=1755552796, filename='qald_10_train_zero_shot_finetuning.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_train_zero_shot_finetuning.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a4c4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-rJizRWlivNQU8UG1S2xaHeGq', created_at=1755552834, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-dr1APYXN5YGKiwHRDlYqZNTJ', result_files=[], seed=808838300, status='validating_files', trained_tokens=None, training_file='file-PvDgBKossQWNnL5Z3P22ty', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(type='supervised', dpo=None, reinforcement=None, supervised=SupervisedMethod(hyperparameters=SupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3))), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-PvDgBKossQWNnL5Z3P22ty\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  hyperparameters={\"n_epochs\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97de9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 359 inference records to /home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. Given the user question, output only valid JSON in this format:\\n\\n{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nAfter whom is the Riemannian geometry named?\\n\\nCandidate Triples (max 10, numbered):\\n(none)\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[2/2] Wrote 359 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "triples_limit = 0\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::C62pYbhe\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _normalize_triple_entry(entry: Any) -> Any:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        return entry[\"triple\"]\n",
    "    return entry\n",
    "\n",
    "def _first_available_triples(sample: Dict[str, Any], limit: int) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\"retrived_triples_ranked\")\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            seq = sample[k]\n",
    "            out = [_normalize_triple_entry(x) for x in seq[:limit]]\n",
    "            return out\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    \"You are an assistant that converts natural language questions into SPARQL queries for Wikidata. \"\n",
    "    \"Given the user question, output only valid JSON in this format:\\n\\n\"\n",
    "    \"{\\n  \\\"sparql\\\": \\\"SPARQL QUERY HERE\\\"\\n}\"\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "\n",
    "    return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "        triples = _first_available_triples(sample, triples_limit)\n",
    "        triples_str = lists_to_numbered_string(triples) if triples else \"(none)\"\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854cc16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-MzkwmVAsnjeyG43K4jmyTP\n",
      "Batch ID: batch_68a3bb165c248190ac6bac193891483d\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD_10 Zero Shot\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    # If you want a dict form:\n",
    "    # print(batch.model_dump())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a954dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_finetuned_plus_gold.json. Total records: 359\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_10/qald_10_test_filtered_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_10_test_zero_shot_finetuned_plus_gold.json\")\n",
    "\n",
    "ANSWER_PREFIX_RE = re.compile(r'<Answer>\\s*', re.DOTALL)\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        first_newline = s.find(\"\\n\")\n",
    "        if first_newline != -1:\n",
    "            s = s[first_newline+1:]\n",
    "        if s.endswith(\"```\"):\n",
    "            s = s[:-3]\n",
    "    return s.strip()\n",
    "\n",
    "def _first_json_object(s: str):\n",
    "    dec = json.JSONDecoder()\n",
    "    i = 0\n",
    "    while True:\n",
    "        start = s.find(\"{\", i)\n",
    "        if start == -1:\n",
    "            return None\n",
    "        try:\n",
    "            obj, end = dec.raw_decode(s[start:])\n",
    "            return obj\n",
    "        except json.JSONDecodeError:\n",
    "            i = start + 1\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    m = ANSWER_PREFIX_RE.search(content)\n",
    "    if m:\n",
    "        content = content[m.end():]\n",
    "\n",
    "    content = _strip_code_fences(content)\n",
    "\n",
    "    # If it's already a pure JSON object, parse directly\n",
    "    s = content.strip()\n",
    "    if s.startswith(\"{\"):\n",
    "        try:\n",
    "            data = json.loads(s)\n",
    "            return data.get(\"sparql\", \"\") if isinstance(data, dict) else \"\"\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # fall through to scanning\n",
    "\n",
    "    # Otherwise, scan the string to find the first JSON object and parse it\n",
    "    data = _first_json_object(content)\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"sparql\", \"\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        cid = rec.get(\"custom_id\")\n",
    "        resp = rec.get(\"response\") or {}\n",
    "        status = resp.get(\"status_code\")\n",
    "        err = rec.get(\"error\")\n",
    "\n",
    "        # Skip errored/unsuccessful results\n",
    "        if err is not None or status != 200:\n",
    "            pred_lookup[cid] = \"\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            content = resp[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pred_lookup[cid] = \"\"\n",
    "            continue\n",
    "\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742193f",
   "metadata": {},
   "source": [
    "QALD-9-Random-Dynamic-Few-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d1487",
   "metadata": {},
   "source": [
    "1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a874cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho was Tom Hanks married to?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Rita_Wilson dbo:spouse res:Tom_Hanks\\n2. res:Rita_Wilson dbp:spouse res:Tom_Hanks\\n3. res:Tom_Hanks dbo:spouse res:Rita_Wilson\\n4. res:Tom_Hanks dbp:spouse res:Rita_Wilson\\n5. res:Tom_Hanks dbo:wikiPageWikiLink res:File:Tom_Hanks_and_wife_Rita_Wilson_836.jpg\\n6. res:Marriage_plot dbo:wikiPageWikiLink res:Tom_Hanks\\n7. res:My_Big_Fat_Greek_Wedding dbo:producer res:Tom_Hanks\\n8. res:My_Big_Fat_Greek_Wedding dbo:wikiPageWikiLink res:Tom_Hanks\\n9. res:\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 1\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        # Be generous about where the demo triples might be\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ff5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-RHtnR6EUMW6m7TYBoU8E3X\n",
      "Batch ID: batch_68a4ecbe0140819086f20a4f7d8371ad\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Download results\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276cd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_1_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9eae75",
   "metadata": {},
   "source": [
    "3 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e78236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho was Tom Hanks married to?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Rita_Wilson dbo:spouse res:Tom_Hanks\\n2. res:Rita_Wilson dbp:spouse res:Tom_Hanks\\n3. res:Tom_Hanks dbo:spouse res:Rita_Wilson\\n4. res:Tom_Hanks dbp:spouse res:Rita_Wilson\\n5. res:Tom_Hanks dbo:wikiPageWikiLink res:File:Tom_Hanks_and_wife_Rita_Wilson_836.jpg\\n6. res:Marriage_plot dbo:wikiPageWikiLink res:Tom_Hanks\\n7. res:My_Big_Fat_Greek_Wedding dbo:producer res:Tom_Hanks\\n8. res:My_Big_Fat_Greek_Wedding dbo:wikiPageWikiLink res:Tom_Hanks\\n9. res:\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    # dict with 'triple'\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    # dict with s/p/o or subject/predicate/object\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    \"\"\"\n",
    "    Produce a numbered list with only 's p o' per line, max 'limit' items.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Be liberal about where triples might live.\n",
    "    Keeps original order; slicing happens later.\n",
    "    \"\"\"\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\",\n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "# ---------------- prompt scaffolding ----------------\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        # Be generous about where the demo triples might be\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1f6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-P2CBXL12epZxAPxSsx5ywh\n",
      "Batch ID: batch_68a4eed6e97081909b5f6c084e945091\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ab0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_3_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c55e3",
   "metadata": {},
   "source": [
    "5 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b301132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho was Tom Hanks married to?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Rita_Wilson dbo:spouse res:Tom_Hanks\\n2. res:Rita_Wilson dbp:spouse res:Tom_Hanks\\n3. res:Tom_Hanks dbo:spouse res:Rita_Wilson\\n4. res:Tom_Hanks dbp:spouse res:Rita_Wilson\\n5. res:Tom_Hanks dbo:wikiPageWikiLink res:File:Tom_Hanks_and_wife_Rita_Wilson_836.jpg\\n6. res:Marriage_plot dbo:wikiPageWikiLink res:Tom_Hanks\\n7. res:My_Big_Fat_Greek_Wedding dbo:producer res:Tom_Hanks\\n8. res:My_Big_Fat_Greek_Wedding dbo:wikiPageWikiLink res:Tom_Hanks\\n9. res:\n",
      "[2/2] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "inference_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs.jsonl\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\",\n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a system message that uses up to the first NUM_DEMOS dynamic pairs (if present) as worked examples.\n",
    "    Robust to 'dynamic_pairs' vs 'dynamic_paris'.\n",
    "    \"\"\"\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    with open(inference_jsonl_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for rec in jsonl_rows:\n",
    "            f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "    print(f\"[1/2] Wrote {len(jsonl_rows)} inference records to {inference_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "    count = 0\n",
    "    with open(inference_jsonl_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, line in enumerate(fin):\n",
    "            messages = json.loads(line)[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[2/2] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4452bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-FU2UQbVswkXAgbrwGgyB9v\n",
      "Batch ID: batch_68a4f71d08688190956d9bfd04f1e9c1\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c36335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_5_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c497d62",
   "metadata": {},
   "source": [
    "LcQUAD-Random-Dynamic-Few-Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae72ff1",
   "metadata": {},
   "source": [
    "1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e0828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 1\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        # Be generous about where the demo triples might be\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4003ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-XKD7QZrFfXFRomEmTAAJHJ\n",
      "Batch ID: batch_68a5467f5b2881909fe5923e530d900f\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93548508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_1_random_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_1_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19a724",
   "metadata": {},
   "source": [
    "3 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1893a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        # Be generous about where the demo triples might be\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d72ebd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-1yE51dTNmH85p9kMJCpJ2T\n",
      "Batch ID: batch_68a54ad0cc8081908655517ee7949e00\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821d1fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_3_random_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_3_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8b4ae",
   "metadata": {},
   "source": [
    "5 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aca08394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        # Be generous about where the demo triples might be\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796a26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-1KyWeZgJ9aqzMwkcEYWyCc\n",
      "Batch ID: batch_68a54dbba6208190b9565a42b80db834\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_random_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a80a85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_5_random_pairs_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_random_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_random_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_test_solo_stage_10_plus_5_random_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebc8be",
   "metadata": {},
   "source": [
    "FT-with Triples-Zero Shot QALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6601288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nExample OUTPUT (your response must follow **this exact shape**)\\n\\n<Answer>\\n{\\\"sparql\\\": \\\"PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nWhat is the time zone of Salt Lake City?\"\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "triples_limit = 0\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\"\n",
    "output_path = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot.jsonl\"\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    'Given a specific question generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "jsonl_rows = []\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    question = sample[\"question\"]\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\"\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 0 else system_msg_generic\n",
    "\n",
    "    jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rec in jsonl_rows:\n",
    "        f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(jsonl_rows)} inference records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e1b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SOURCE  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot.jsonl\"\n",
    "TARGET  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_input.jsonl\"\n",
    "MODEL   = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "with open(SOURCE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(TARGET,  \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        messages = json.loads(line)[\"messages\"]\n",
    "\n",
    "        batch_row = {\n",
    "            \"custom_id\": f\"example_{idx}\",\n",
    "            \"method\":    \"POST\",\n",
    "            \"url\":       \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\":       MODEL,\n",
    "                \"messages\":    messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "        }\n",
    "        fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {idx+1} lines to {TARGET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b520abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-F6cwg9JkQAD5XmsCKQ5QVn\n",
      "Batch ID: batch_68af439b8da881909c32230da5cd540a\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs to qald_test_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs to qald_test_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ee6b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0fd30e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 0\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_gold_dynamic_pairs.json\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dffe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-DnecKnopLTVFwdFVZSF9vJ\n",
      "Batch ID: batch_68af4c268b3c81908ab976e19fb90847\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QaLD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe6f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_gold_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_plus_5_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831f46e",
   "metadata": {},
   "source": [
    "FT - without Triples-Zero Shot QALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae65d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 406 training records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_zero_shot.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nExample OUTPUT (your response must follow **this exact shape**)\\n\\n<Answer>\\n{\\\"sparql\\\": \\\"PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nWhich people were born in Heraklion?\"\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "input_path  = '/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_train_first_stage_ranked.json'\n",
    "output_path = '/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_zero_shot.jsonl'\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    \"Given a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, \"\n",
    "    'in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic   = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "def sparql_formatter(raw_query: str) -> str:\n",
    "    one_line = ' '.join(raw_query.strip().split())\n",
    "    return \"<Answer>\\n\" + json.dumps({\"sparql\": one_line}, ensure_ascii=False)\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset[2:], start=2):\n",
    "    question = sample['question']\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\"\n",
    "    }\n",
    "\n",
    "    gold_query = sample['formated_query']\n",
    "    assistant_msg = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": sparql_formatter(gold_query)\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 2 else system_msg_generic\n",
    "\n",
    "    new_dataset.append({\n",
    "        \"messages\": [system_msg, user_msg, assistant_msg]\n",
    "    })\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "    for record in new_dataset:\n",
    "        f_out.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Wrote {len(new_dataset)} training records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(new_dataset[0], indent=2)[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b64366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-LAUAmcXPS73kQTfvM5WZyC', bytes=195124, created_at=1756332347, filename='qald_train_zero_shot.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_zero_shot.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791a3807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-b8NDapmw2MgdusgQtOHsUtqS', created_at=1756332363, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-dr1APYXN5YGKiwHRDlYqZNTJ', result_files=[], seed=1856328388, status='validating_files', trained_tokens=None, training_file='file-LAUAmcXPS73kQTfvM5WZyC', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(type='supervised', dpo=None, reinforcement=None, supervised=SupervisedMethod(hyperparameters=SupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3))), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-LAUAmcXPS73kQTfvM5WZyC\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  hyperparameters={\"n_epochs\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f20d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_second_time.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nExample OUTPUT (your response must follow **this exact shape**)\\n\\n<Answer>\\n{\\\"sparql\\\": \\\"PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\\n\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Question:\\nWhat is the time zone of Salt Lake City?\"\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "triples_limit = 0\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\"\n",
    "output_path = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_second_time.jsonl\"\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    'Given a specific question generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "jsonl_rows = []\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    question = sample[\"question\"]\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\"\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 0 else system_msg_generic\n",
    "\n",
    "    jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rec in jsonl_rows:\n",
    "        f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(jsonl_rows)} inference records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47ca5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SOURCE  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_with_triples_zero_shot_second_time.jsonl\"\n",
    "TARGET  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_input.jsonl\"\n",
    "MODEL   = \"ft:gpt-3.5-turbo-0125:personal::C9J5ld48\"\n",
    "\n",
    "with open(SOURCE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(TARGET,  \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        messages = json.loads(line)[\"messages\"]\n",
    "\n",
    "        batch_row = {\n",
    "            \"custom_id\": f\"example_{idx}\",\n",
    "            \"method\":    \"POST\",\n",
    "            \"url\":       \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\":       MODEL,\n",
    "                \"messages\":    messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "        }\n",
    "        fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {idx+1} lines to {TARGET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a330efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-CVx6QuKHjgiDGtwKVK7sFr\n",
      "Batch ID: batch_68f6fcd0e4808190b1b67787b69a3839\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs to qald_test_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs to qald_test_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_second_time_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c13f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 0\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_gold_dynamic_pairs.json\"\n",
    "\n",
    "batch_jsonl_path = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::C9J5ld48\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632eba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-FD8AYCGnF7iQJ7d2RJgL3e\n",
      "Batch ID: batch_68f70580c9388190b6c7fe2a74b1bb5a\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QaLD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03895a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_gold_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_3_dynamic_pairs_second_time_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c3c12",
   "metadata": {},
   "source": [
    "Solo Stage Top 10 QALD(re-doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "111607e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 406 training records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_solo_stage_top_10_re.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Skype dbo:developer res:Skype_Technologies\\n2. res:21Vianet dbo:service res:Skype\\n3. res:Skype gold:hypernym res:Application\\n4. res:Skype dbp:operatingSystem res:HoloLens\\n5. res:Skype dbo:operatingSystem res:HoloLens\\n6. res:Skype dbp:operatingSystem res:IOS\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "triples_limit = 10\n",
    "input_path  = '/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_train_first_stage_ranked.json'\n",
    "output_path = '/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_solo_stage_top_10_re.jsonl'\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_triples = [\n",
    "    [\"res:Skype\", \"dbo:developer\",        \"res:Skype_Technologies\"],\n",
    "    [\"res:21Vianet\", \"dbo:service\",       \"res:Skype\"],\n",
    "    [\"res:Skype\", \"gold:hypernym\", \"res:Application\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:HoloLens\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:HoloLens\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:IOS\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:IOS\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:IPadOS\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:IPadOS\"],\n",
    "    [\"res:Skype\", \"dbp:license\",          \"res:Proprietary_software\"],\n",
    "]\n",
    "demo_triples_str = lists_to_numbered_string(demo_triples)\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question and up to ten potentially relevant triples, generate the\n",
    "corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Candidate Triples (numbered, max 10):\n",
    "{demo_triples_str}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    \"Given a specific question and up to ten potentially relevant triples, \"\n",
    "    \"generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, \"\n",
    "    'in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic   = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "def sparql_formatter(raw_query: str) -> str:\n",
    "    one_line = ' '.join(raw_query.strip().split())\n",
    "    return \"<Answer>\\n\" + json.dumps({\"sparql\": one_line}, ensure_ascii=False)\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset[2:], start=2):\n",
    "    question = sample['question']\n",
    "    raw_hits = sample.get('retrived_triples_ranked', [])[:triples_limit]\n",
    "    triples = [hit['triple'] for hit in raw_hits]\n",
    "    triples_str = lists_to_numbered_string(triples)\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "    }\n",
    "\n",
    "    gold_query = sample['formated_query']\n",
    "    assistant_msg = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": sparql_formatter(gold_query)\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 2 else system_msg_generic\n",
    "\n",
    "    new_dataset.append({\n",
    "        \"messages\": [system_msg, user_msg, assistant_msg]\n",
    "    })\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "    for record in new_dataset:\n",
    "        f_out.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Wrote {len(new_dataset)} training records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(new_dataset[0], indent=2)[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8a2c6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-2yTGV8g3dWmc7N62C5Zs5X', bytes=468441, created_at=1756350333, filename='qald_train_solo_stage_top_10_re.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_train_solo_stage_top_10_re.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "514894f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-QYGcvNxBbcc64Bnxjn9ub8kP', created_at=1756350355, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-dr1APYXN5YGKiwHRDlYqZNTJ', result_files=[], seed=1117803976, status='validating_files', trained_tokens=None, training_file='file-2yTGV8g3dWmc7N62C5Zs5X', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(type='supervised', dpo=None, reinforcement=None, supervised=SupervisedMethod(hyperparameters=SupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=3))), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-2yTGV8g3dWmc7N62C5Zs5X\",\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  hyperparameters={\"n_epochs\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a424bb",
   "metadata": {},
   "source": [
    "QALD Solo Stage Top 10 Inference Second Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a223c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 inference records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Skype dbo:developer res:Skype_Technologies\\n2. res:21Vianet dbo:service res:Skype\\n3. res:Skype gold:hypernym res:Application\\n4. res:Skype dbp:operatingSystem res:HoloLens\\n5. res:Skype dbo:operatingSystem res:HoloLens\\n6. res:Skype dbp:operatingSystem res:IOS\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "triples_limit = 10\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\"\n",
    "output_path = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re.jsonl\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_triples = [\n",
    "    [\"res:Skype\", \"dbo:developer\",        \"res:Skype_Technologies\"],\n",
    "    [\"res:21Vianet\", \"dbo:service\",       \"res:Skype\"],\n",
    "    [\"res:Skype\", \"gold:hypernym\",        \"res:Application\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:HoloLens\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:HoloLens\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:IOS\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:IOS\"],\n",
    "    [\"res:Skype\", \"dbp:operatingSystem\",  \"res:IPadOS\"],\n",
    "    [\"res:Skype\", \"dbo:operatingSystem\",  \"res:IPadOS\"],\n",
    "    [\"res:Skype\", \"dbp:license\",          \"res:Proprietary_software\"],\n",
    "]\n",
    "demo_triples_str = lists_to_numbered_string(demo_triples)\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question and up to ten potentially relevant triples, generate the\n",
    "corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Candidate Triples (numbered, max 10):\n",
    "{demo_triples_str}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "jsonl_rows = []\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    question = sample[\"question\"]\n",
    "    raw_hits_1  = [hit[\"triple\"] for hit in sample[\"retrived_triples_ranked\"][:triples_limit]]\n",
    "    triples_str = lists_to_numbered_string(raw_hits_1)\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 0 else system_msg_generic\n",
    "\n",
    "    jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rec in jsonl_rows:\n",
    "        f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(jsonl_rows)} inference records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82b178fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SOURCE  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re.jsonl\"\n",
    "TARGET  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re_batch_input.jsonl\"\n",
    "MODEL   = \"ft:gpt-3.5-turbo-0125:personal::C9Nme00Y\"\n",
    "\n",
    "with open(SOURCE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(TARGET,  \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        messages = json.loads(line)[\"messages\"]\n",
    "\n",
    "        batch_row = {\n",
    "            \"custom_id\": f\"example_{idx}\",\n",
    "            \"method\":    \"POST\",\n",
    "            \"url\":       \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\":       MODEL,\n",
    "                \"messages\":    messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "        }\n",
    "        fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {idx+1} lines to {TARGET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c17cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-YE7bfXF6jrc5Ryy2txKXi4\n",
      "Batch ID: batch_68afd4fb6c448190bd8969512b61c710\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs to qald_test_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_re_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_re_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs to qald_test_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a91a8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_re_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/qald_9_plus/first_stage_ranker/qald_test_first_stage_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_re_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_re_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93567b",
   "metadata": {},
   "source": [
    "Dynamic Few Shot on new Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2580e190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_plus_5_random_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_random_dynamic_pairs.json\"\n",
    "\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_plus_5_random_dynamic_pairs_batch_input.jsonl\"\n",
    "\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::C9Nme00Y\"\n",
    "\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, rec in enumerate(jsonl_rows):\n",
    "            messages = rec[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b630d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_plus_5_dynamic_pairs_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_plus_5_dynamic_pairs_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbb1353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_re_plus_1_random_dynamic_pairs_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_random_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_re_plus_1_random_dynamic_pairs_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_test_solo_stage_10_re_plus_1_random_dynamic_pairs_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe44513",
   "metadata": {},
   "source": [
    "Vquanda soloStage top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b22678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 inference records to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\nGiven a specific question and up to three potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWho developed Skype?\\n\\nCandidate Triples (numbered, max 3):\\n1. res:Skype dbo:developer res:Skype_Technologies\\n2. res:21Vianet dbo:service res:Skype\\n3. res:Skype gold:hypernym res:Application\\n\\nExample OUTPUT (your response must follow **this exact shape**)\\n\\n<Answer>\\n{\\\"sparql\\\": \\\"PREFIX dbo: <http://dbpedia.org/ontolog\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "triples_limit = 3\n",
    "input_path  = \"/home/m2khoda/dual_retriever/datasets/vquanda/vquanda_test_ranked.json\"\n",
    "output_path = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3.jsonl\"\n",
    "\n",
    "def lists_to_numbered_string(triples: List[Any]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {' '.join(map(str, t)) if isinstance(t, (list, tuple)) else str(t)}\"\n",
    "        for i, t in enumerate(triples, 1)\n",
    "    )\n",
    "\n",
    "demo_question = \"Who developed Skype?\"\n",
    "demo_triples = [\n",
    "    [\"res:Skype\", \"dbo:developer\",        \"res:Skype_Technologies\"],\n",
    "    [\"res:21Vianet\", \"dbo:service\",       \"res:Skype\"],\n",
    "    [\"res:Skype\", \"gold:hypernym\",        \"res:Application\"],\n",
    "]\n",
    "demo_triples_str = lists_to_numbered_string(demo_triples)\n",
    "demo_answer = (\n",
    "    \"<Answer>\\n\"\n",
    "    \"{\\\"sparql\\\": \"\n",
    "    \"\\\"PREFIX dbo: <http://dbpedia.org/ontology/> \"\n",
    "    \"PREFIX res: <http://dbpedia.org/resource/> \"\n",
    "    \"SELECT DISTINCT ?uri WHERE { res:Skype dbo:developer ?uri }\\\"}\"\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT_WITH_DEMO = f\"\"\"\n",
    "Given a specific question and up to three potentially relevant triples, generate the\n",
    "corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\n",
    "with key \"sparql\" and the query as its string value.\n",
    "\n",
    "Example INPUT (exactly what you will receive for every task)\n",
    "\n",
    "Question:\n",
    "{demo_question}\n",
    "\n",
    "Candidate Triples (numbered, max 3):\n",
    "{demo_triples_str}\n",
    "\n",
    "Example OUTPUT (your response must follow **this exact shape**)\n",
    "\n",
    "{demo_answer}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GENERIC = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "system_msg_with_demo = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_WITH_DEMO}\n",
    "system_msg_generic = {\"role\": \"system\", \"content\": SYSTEM_PROMPT_GENERIC}\n",
    "\n",
    "jsonl_rows = []\n",
    "\n",
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for idx, sample in enumerate(dataset):\n",
    "    question = sample[\"question\"]\n",
    "    raw_hits_1  = [hit[\"triple\"] for hit in sample[\"retrieved_triples_ranked\"][:triples_limit]]\n",
    "    triples_str = lists_to_numbered_string(raw_hits_1)\n",
    "\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 3, numbered):\\n{triples_str}\"\n",
    "    }\n",
    "\n",
    "    system_msg = system_msg_with_demo if idx == 0 else system_msg_generic\n",
    "\n",
    "    jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rec in jsonl_rows:\n",
    "        f_out.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(jsonl_rows)} inference records to {output_path}\")\n",
    "print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fad64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SOURCE  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3.jsonl\"\n",
    "TARGET  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_batch_input.jsonl\"\n",
    "MODEL   = \"ft:gpt-3.5-turbo-0125:personal::CSv7fg5V\"\n",
    "\n",
    "with open(SOURCE, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(TARGET,  \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        messages = json.loads(line)[\"messages\"]\n",
    "\n",
    "        batch_row = {\n",
    "            \"custom_id\": f\"example_{idx}\",\n",
    "            \"method\":    \"POST\",\n",
    "            \"url\":       \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\":       MODEL,\n",
    "                \"messages\":    messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "        }\n",
    "        fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {idx+1} lines to {TARGET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7efcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-NwtQwVN7zb6KTVvHLa7HqT\n",
      "Batch ID: batch_68f6f7c8859481909c69e64e171fd506\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs to qald_test_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"Vquanda test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    # batch.error no longer exists—just dump the batch object\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    # If you want a dict form:\n",
    "    # print(batch.model_dump())\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# 4️⃣ Download results\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "# use the new helper name ↓\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs to qald_test_batch_output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0efb00b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/datasets/vquanda/vquanda_test_ranked.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/extra_experiments/vquanda_test_gpt_top_3_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce9869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
