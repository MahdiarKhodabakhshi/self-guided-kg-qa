{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364dfd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 150/150 [00:00<00:00, 31383.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 745 requests to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0561d730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-MJ7qmrGcU3L8GNpffQY6yd\n",
      "Batch ID: batch_68a4ebf7681081908ef2603f60a1c7d5\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c04687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 745/745 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads OpenAI Batch result .jsonl and returns: custom_id -> assistant content (or error text)\n",
    "    \"\"\"\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Heuristic: find the list field that holds the entries.\n",
    "    Prefer lists whose elements have 'dynamic_pairs'.\n",
    "    Fallback to the first list-of-dicts field.\n",
    "    \"\"\"\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e3828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 1\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9638bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-QPu1RvcmFQHECxH7DFjsoU\n",
      "Batch ID: batch_68a4f82556cc8190bcf0f86632a8549c\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01607ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aaa5f5",
   "metadata": {},
   "source": [
    "3 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf6e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca11a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-RTmZTxxye4Jtx3VRQc3jRP\n",
      "Batch ID: batch_68a4f9e639a08190bf1c2c98f9c22161\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f7ed933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121756ee",
   "metadata": {},
   "source": [
    "5 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "512e3915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Bk9BchWy\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ede425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-7a4MCaBz8oX9WJojwUvLmQ\n",
      "Batch ID: batch_68a4fed4775c81909b8f736eef3d021d\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c69963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53498a",
   "metadata": {},
   "source": [
    "LcQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb40f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 28613.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 4955 requests to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b31834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6195367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 4954/4955 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a4b6cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writer\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 1\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86ac3786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-XEtwz3XHDuMbE2vbaq9oLL\n",
      "Batch ID: batch_68a5530e86a48190a09b9d52b797dba5\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b5c71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_1_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec0c3d",
   "metadata": {},
   "source": [
    "3 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cba23bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writer\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any):\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "204581af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-Q2nbpXckrNQo9xkWS7zRAx\n",
      "Batch ID: batch_68a55831cd48819091b4b5b1da2cbe73\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c08bc519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_3_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79f847",
   "metadata": {},
   "source": [
    "5 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57db3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 1000 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhich writer of Tales of Suspense is also the writer of karakuri Dji Ultimo ?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:Tales_of_Suspense dbo:wikiPageWikiLink res:James_Robinson_(writer)\\n2. res:James_Robinson_(writer) dbo:wikiPageWikiLink res:Tales_of_Suspense\\n3. res:Tales_of_Suspense dbo:writer res:Larry_Lieber\\n4. res:Tales_of_Suspense dbo:writer res:Robert_Bernstein_(comics)\\n5. res:Tales_of_Suspense dbo:writer res:Stan_Lee\\n6. res:Tales_of_Suspense dbp:writers res:Larry_Lieber\\n7. res:Tales_of_Suspense dbp:writer\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 5\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_dynamic_pairs_with_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::Br5K42ie\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any):\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()  # Get the COT here\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "142cb536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-KEtS9QDUXJf5pnhDM2fxNV\n",
      "Batch ID: batch_68a55dcd654c8190a6833b2069ddcbc3\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"LcQUAD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a2efc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_plus_gold.json. Total records: 1000\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_gold_with_dynamic_pairs.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_plus_5_dynamic_pairs_with_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9ed9d",
   "metadata": {},
   "source": [
    "LcQUAD-Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ef509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 23632.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_pairs_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0c18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-4GVdfxGzZJsXZ6nNamDmyp\n",
      "Batch ID: batch_68a8a8d051d4819097d17d92229a0b89\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523537e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 5000/5000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dbf43",
   "metadata": {},
   "source": [
    "QALD-Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7178c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 150/150 [00:00<00:00, 32643.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 750 requests to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4f0592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-LiTBLvkRoTinGsV4SKYGYR\n",
      "Batch ID: batch_68a8b218d8b08190a990806c67a35705\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1b7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 750/750 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a3a3b",
   "metadata": {},
   "source": [
    "LcQUAD-Mistral 7B-v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eab2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 25242.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a0f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-Q3JTa4KqZ76z5ir63q1RxQ\n",
      "Batch ID: batch_68a9e185217c8190b0cc1d35fda0efaf\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_gold_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7b4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 5000/5000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/lcquad_test_solo_stage_10_mistral_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ba0ef",
   "metadata": {},
   "source": [
    "Mixtral 8x7-v01----------------QALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b285bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 150/150 [00:00<00:00, 26812.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 750 requests to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,  # Added max_tokens here\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f985ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-WBukewuTiLoE4F691Rd1bd\n",
      "Batch ID: batch_68aaba65f53c81908140d3f03455c5ec\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e7f259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 750/750 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/mixtral/qald_test_solo_stage_10_mixtral_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00904b",
   "metadata": {},
   "source": [
    "Mixtral 8x7-v01----------------LcQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db37533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 26259.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1-nano\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant who understands the SPARQL Protocol and RDF Query Language.\n",
    "Given a natural-language question, its gold triples, and the correct SPARQL query, explain in a few sentences\n",
    "how the SPARQL query is derived from the question using the gold triples. Focus on showing the reasoning steps\n",
    "that connect the question to the structure of the SPARQL query. Keep it concise (2–4 sentences).\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"**Example 1**\n",
    "Q: How many movies did Stanley Kubrick direct?\n",
    "Gold triple: <?uri, director, Stanley_Kubrick>\n",
    "SPARQL:\n",
    "SELECT DISTINCT COUNT(?uri) WHERE {\n",
    "  ?uri <http://dbpedia.org/ontology/director> <http://dbpedia.org/resource/Stanley_Kubrick> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for the number of movies directed by Stanley Kubrick. The gold triple links movies (?uri) with Stanley Kubrick via the director relation. Since we need a count of movies, the query uses COUNT on distinct ?uri.\n",
    "\n",
    "**Example 2**\n",
    "Q: Who won the Lovelace Medal and the Norbert Wiener Award for Social and Professional Responsibility?\n",
    "Gold triples:\n",
    "- <?uri, prizes, Lovelace_Medal>\n",
    "- <?uri, prizes, Norbert_Wiener_Award_for_Social_and_Professional_Responsibility>\n",
    "SPARQL:\n",
    "SELECT DISTINCT ?uri WHERE {\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Lovelace_Medal> .\n",
    "  ?uri <http://dbpedia.org/property/prizes> <http://dbpedia.org/resource/Norbert_Wiener_Award_for_Social_and_Professional_Responsibility> .\n",
    "}\n",
    "\n",
    "Expected output:\n",
    "The question asks for a person who has both awards. The first triple retrieves people with the Lovelace Medal, and the second triple restricts to those who also have the Norbert Wiener Award. Using both triples together finds the intersection, and DISTINCT avoids duplicates.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the concise explanation (2–4 sentences), no bullets, no code, no extra headings.\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "Gold triples:\n",
    "{triples_str}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\"\"\"\n",
    "\n",
    "MODEL       = \"gpt-4.1-nano\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            # REQUIRED: custom_id + method + url + body\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b354bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-SoDcw2Er6WazDdSFBfsiQs\n",
      "Batch ID: batch_68ab5174d3c081908442c22782503c79\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8422ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 5000/5000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_10_mixtral_plus_dynamic_pairs_with_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_top_10_mixtral_with_dynamic_cot_pairs_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/mixtral/lcquad_test_solo_stage_10_mixtral_plus_dynamic_pairs_with_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ad096",
   "metadata": {},
   "source": [
    "New Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20bdaa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 75/75 [00:00<00:00, 25424.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 370 requests to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items[75:]  # for testing, limit to first 100 items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results//home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_batch_output_from_75.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8a6daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 745/745 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_plus_gold.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_plus_gold.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ff03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_new_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question and up to ten potentially relevant triples, generate the\\ncorresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\nwith key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nCandidate Triples (numbered, max 10):\\n1. res:San_Pedro_de_Atacama dbo:timeZone res:Time_in_Chile\\n2. res:San_Pedro_de_Atacama dbp:timezone res:Time_in_Chile\\n3. res:San_Pedro_de_Atacama dbo:wikiPageWikiLink res:Time_in_Chile\\n4. res:2021_AV7 dbp:discoverySite res:San_Pedro_de_Atacama\\n5. res:2021_AV7 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n6. res:1577 dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n7. res:2021_in_sports dbo:wikiPageWikiLink res:San_Pedro_de_Atacama\\n8. r\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "triples_limit = 10\n",
    "NUM_DEMOS = 1\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_plus_gold.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_new_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::C9Nme00Y\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "def _coerce_triple(entry: Any) -> Union[str, List[str]]:\n",
    "    if isinstance(entry, dict) and \"triple\" in entry:\n",
    "        entry = entry[\"triple\"]\n",
    "\n",
    "    if isinstance(entry, dict):\n",
    "        if {\"s\", \"p\", \"o\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"s\"]), str(entry[\"p\"]), str(entry[\"o\"])]\n",
    "        if {\"subject\", \"predicate\", \"object\"} <= set(entry.keys()):\n",
    "            return [str(entry[\"subject\"]), str(entry[\"predicate\"]), str(entry[\"object\"])]\n",
    "\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) == 3:\n",
    "        return [str(entry[0]), str(entry[1]), str(entry[2])]\n",
    "\n",
    "    if isinstance(entry, str):\n",
    "        return entry.strip()\n",
    "\n",
    "    return str(entry)\n",
    "\n",
    "def _format_triples_for_prompt(seq: List[Any], limit: int) -> str:\n",
    "    lines: List[str] = []\n",
    "    for i, raw in enumerate(seq[:limit], 1):\n",
    "        t = _coerce_triple(raw)\n",
    "        if isinstance(t, str):\n",
    "            triple_str = t\n",
    "        else:\n",
    "            triple_str = \" \".join(map(str, t))\n",
    "        lines.append(f\"{i}. {triple_str}\")\n",
    "    return \"\\n\".join(lines) if lines else \"(none)\"\n",
    "\n",
    "def _get_triple_candidates(sample: Dict[str, Any]) -> List[Any]:\n",
    "    candidate_keys: Iterable[str] = (\n",
    "        \"retrived_triples_ranked\", \n",
    "        \"retrieved_triples_ranked\",\n",
    "        \"retrieved_triples_top10\",\n",
    "        \"retrieved_triples\",\n",
    "        \"triples\",\n",
    "    )\n",
    "    for k in candidate_keys:\n",
    "        if k in sample and sample[k]:\n",
    "            return sample[k]\n",
    "    return []\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question and up to ten potentially relevant triples, '\n",
    "    'generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()\n",
    "\n",
    "        demo_triples_seq = (\n",
    "            demo.get(\"retrieved_triples_top10\")\n",
    "            or demo.get(\"retrived_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples_ranked\")\n",
    "            or demo.get(\"retrieved_triples\")\n",
    "            or demo.get(\"triples\")\n",
    "            or []\n",
    "        )\n",
    "        demo_triples_str = _format_triples_for_prompt(demo_triples_seq, triples_limit)\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Candidate Triples (numbered, max 10):\\n{demo_triples_str}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question and up to ten potentially relevant triples, generate the\\n\"\n",
    "        \"corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON\\n\"\n",
    "        'with key \"sparql\" and the query as its string value.\\n\\n'\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        triples_seq = _get_triple_candidates(sample)\n",
    "        triples_str = _format_triples_for_prompt(triples_seq, triples_limit)\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nCandidate Triples (max 10, numbered):\\n{triples_str}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea09767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_new_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_1_dynamic_pairs_with_new_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a3cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_new_cot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/qald_test_solo_stage_10_plus_dynamic_pairs_new_cot_plus_gold.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_new_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/qald_test_solo_stage_10_plus_5_dynamic_pairs_with_new_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c9fc9",
   "metadata": {},
   "source": [
    "CodeLlama - QALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00488478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 150/150 [00:00<00:00, 25883.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 750 requests to /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c400b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-KwFU9x7CYQkKSALdVJHDaa\n",
      "Batch ID: batch_68b70d0eb36c8190aa743cd2a690650a\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"QALD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b22ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 750/750 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_plus_gold.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evaluation/qald_results/codellama/qald_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_plus_gold.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623b48b",
   "metadata": {},
   "source": [
    "Codellama - LcQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fa6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 7617.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbd6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-FWQWMz8s4kckeNq1kRWe3T\n",
      "Batch ID: batch_68b70e33a5808190a6a5ee075e85e1ea\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d3fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 4999/5000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_plus_gold.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/lcquad_results/codellama/lcquad_test_solo_stage_10_codellama_plus_dynamic_pairs_new_cot_plus_gold.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c9835",
   "metadata": {},
   "source": [
    "Vquanda - CodeLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc7a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 16008.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_cot_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa66e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-RqhBqqenT1YxAYZSfmChik\n",
      "Batch ID: batch_68dd8c9de71481908593f5ba4567e82f\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a1b69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 5000/5000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/codellama/vquanda_test_codellama_top_10_codellama_plus_dynamic_pairs_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4871af",
   "metadata": {},
   "source": [
    "Vquanda - Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fada8bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 29688.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3000 requests to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/vquanda/vquanda_test_mixtral_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/vquanda/vquanda_test_mixtral_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0e0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-FgvZvSpt4EFpW9c539kShF\n",
      "Batch ID: batch_68deb09f2aa88190a52e982d733e601c\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs_new_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/vquanda/vquanda_test_mixtral_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3df4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 3000/3000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs_new_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/mixtral/vquanda_test_mixtral_top_10_plus_dynamic_pairs_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a720d9",
   "metadata": {},
   "source": [
    "Vquanda - GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3d10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 1000/1000 [00:00<00:00, 26086.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3000 requests to /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af4328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-49maACkUMd6hVxjPGuwXRC\n",
      "Batch ID: batch_68e2b955a62481908074b45fd6b35961\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e39304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 3000/3000 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/dycot/vquanda_results/gpt/vquanda_test_gpt_top_10_plus_dynamic_pairs_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876bccc",
   "metadata": {},
   "source": [
    "Zero Shot GPT QALD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df685238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building batch JSONL: 100%|██████████| 150/150 [00:00<00:00, 30025.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 740 requests to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import openai\n",
    "\n",
    "INPUT_PATH   = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_gold_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_PATH = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "MODEL        = \"gpt-4.1\"\n",
    "TEMPERATURE  = 0.0\n",
    "RESUME       = True \n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert DBpedia/SPARQL explainer.\n",
    "Your job is to generate a chain-of-thought (CoT) style explanation for a given question and its corresponding DBpedia SPARQL.\n",
    "Write in the first person (“I …”). When applicable, begin with:\n",
    "- Entity assignment: mapping named entities in the question to their ids (e.g., res:, dbo:)\n",
    "- Predicate ids: briefly name the predicates involved\n",
    "Then, in a few sentences, explain how the SPARQL answers the question.\n",
    "Be precise, technical where helpful, and avoid extra fluff or markdown headings.\"\"\"\n",
    "\n",
    "FEWSHOT_EXAMPLES = \"\"\"\n",
    "Your task is to generate a chain-of-thought (CoT) for a pair of question and its corresponding DBPedia SPARQL. Below are example pairs. For each, provide a detailed, simplified explanation written from the perspective of \"I\". Use technical terms where helpful. For each example, first specify which entity ids are assigned to each named entity mentioned in the question. Also, explain the predicate ids used when needed.\n",
    "\n",
    "1. Question: Which countries have places with more than two caves?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?cave rdf:type dbo:Cave ; dbo:location ?uri . ?uri rdf:type dbo:Country } GROUP BY ?uri HAVING ( COUNT(?cave) > 2 )\n",
    "   Expected Output: Entity assignment: 'dbo:Cave' for caves, 'dbo:Country' for countries. Predicate ids: 'dbo:location' gives the location of the cave. I want to list countries that have more than two caves. I use SELECT DISTINCT to get unique country results. I first match all entities that are caves (?cave, where rdf:type dbo:Cave), then get their locations via dbo:location (?uri). Then, I ensure those locations are countries (?uri rdf:type dbo:Country). By grouping the results by country using GROUP BY and adding HAVING (COUNT(?cave) > 2), I ensure I only get countries with more than two caves.\n",
    "\n",
    "2. Question: What is the longest river?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River { ?uri dbo:length ?l } UNION { ?uri dbp:length ?l } } ORDER BY DESC(?l) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers. Predicate ids: 'dbo:length', 'dbp:length' for the river length. To find the longest river, I select all rivers with ?uri a dbo:River and retrieve their lengths, checking both dbo:length and dbp:length predicates. I use UNION to ensure all lengths are covered. Then, I order the results from longest to shortest (ORDER BY DESC(?l)), and LIMIT 1 to only get the longest river.\n",
    "\n",
    "3. Question: Do Prince Harry and Prince William have the same parents?\n",
    "   SPARQL: PREFIX dbo: <http://dbpedia.org/ontology/> PREFIX res: <http://dbpedia.org/resource/> ASK WHERE { <http://dbpedia.org/resource/Prince_William,_Duke_of_Cambridge> dbo:parent ?x . res:Prince_Harry dbo:parent ?x }\n",
    "   Expected Output: Entity assignment: 'res:Prince_Harry' and 'res:Prince_William,_Duke_of_Cambridge' for Prince Harry and Prince William. Predicate id: 'dbo:parent' is the parent relationship. This question asks if both Prince Harry and Prince William share the same parents. I use the ASK keyword for a true/false answer. I check if there is any parent (?x) linked to both Prince Harry and Prince William. If such a parent exists, the SPARQL query returns true.\n",
    "\n",
    "4. Question: Which volcanos in Japan erupted since 2000?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:Volcano ; dbo:locatedInArea res:Japan ; dbo:eruptionYear ?date FILTER ( year(?date) >= 2000 ) }\n",
    "   Expected Output: Entity assignment: 'dbo:Volcano' for volcanos, 'res:Japan' for Japan. Predicate ids: 'dbo:locatedInArea' links the volcano to Japan, 'dbo:eruptionYear' gives the eruption year. I want to find volcanos in Japan that erupted in or after the year 2000. I identify volcanos located in Japan using dbo:locatedInArea res:Japan, and then use dbo:eruptionYear to get their eruption year. I apply a FILTER with year(?date) >= 2000 to only include those since 2000.\n",
    "\n",
    "5. Question: Give me all world heritage sites designated within the past two years.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:WorldHeritageSite . { ?uri dbp:year '2013'^^xsd:integer . } UNION { ?uri dbp:year '2014'^^xsd:integer . } }\n",
    "   Expected Output: Entity assignment: 'dbo:WorldHeritageSite' for world heritage sites. Predicate ids: 'dbp:year' specifies the designation year. To find World Heritage Sites designated in the last two years, I select entities of type dbo:WorldHeritageSite using rdf:type. The pattern '?uri dbp:year '2013'^^xsd:integer' (similarly for '2014') matches sites with a 'dbp:year' property equal to an integer-valued year—here, either 2013 or 2014. The use of UNION allows us to include sites from both years. '^^xsd:integer' ensures that the year value is treated as an integer in the query. DISTINCT avoids duplicates in the results.\n",
    "\n",
    "6. Question: Which rivers flow into a German lake?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri a dbo:River . ?x dbo:inflow ?uri ; a dbo:Lake ; dbo:country res:Germany }\n",
    "   Expected Output: Entity assignment: 'dbo:River' for rivers, 'dbo:Lake' for lakes, 'res:Germany' for Germany. Predicate ids: 'dbo:inflow' connects the river to the lake, 'dbo:country' specifies the lake's country. I want rivers that flow into lakes in Germany. I select entities that are rivers (?uri a dbo:River), then check for lakes (?x) with dbo:inflow connecting to those rivers, and use dbo:country to restrict lakes to Germany. DISTINCT ensures each river only shows up once.\n",
    "\n",
    "7. Question: Give me all actors starring in movies directed by and starring William Shatner.\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?x dbo:director res:William_Shatner ; dbo:starring res:William_Shatner { ?x dbo:starring ?uri } UNION { ?x dbp:starring ?uri } }\n",
    "   Expected Output: Entity assignment: 'res:William_Shatner' for William Shatner. Predicate ids: 'dbo:director' and 'dbo:starring' identify films directed by and starring William Shatner; 'dbp:starring' is an alternative starring predicate. I look for films that William Shatner both directed and acted in. For those films, I select co-stars using dbo:starring and dbp:starring (via UNION). DISTINCT ensures unique actor results.\n",
    "\n",
    "8. Question: Which actor was casted in the most movies?\n",
    "   SPARQL: SELECT DISTINCT ?uri WHERE { ?uri rdf:type dbo:Actor . ?f rdf:type dbo:Film . ?f dbo:starring ?uri . } ORDER BY DESC(COUNT(DISTINCT(?f))) OFFSET 0 LIMIT 1\n",
    "   Expected Output: Entity assignment: 'dbo:Actor' for actors, 'dbo:Film' for films. Predicate id: 'dbo:starring' lists movie cast. To find the most prolific actor, I select all actors (?uri rdf:type dbo:Actor) and the films they've appeared in (?f rdf:type dbo:Film; ?f dbo:starring ?uri). I count the number of different films for each actor, then sort actors in descending order of movie count and use LIMIT 1 to get the one with the most appearances.\n",
    "\n",
    "9. Question: Is Frank Herbert still alive?\n",
    "   SPARQL: PREFIX dbo: http://dbpedia.org/ontology/ PREFIX res: http://dbpedia.org/resource/ ASK WHERE { OPTIONAL { res:Frank_Herbert dbo:deathDate ?date } FILTER ( ! bound(?date) ) }\n",
    "   Expected Output: Entity assignment: 'res:Frank_Herbert' for Frank Herbert. Predicate id: 'dbo:deathDate' gives the individual's death date. To check if Frank Herbert is alive, I use ASK for a yes/no result. I do an OPTIONAL lookup for his dbo:deathDate. If there's no death date (i.e., the variable is unbound), the query returns true, meaning he's still alive.\n",
    "\"\"\"\n",
    "\n",
    "ITEM_INSTRUCTIONS = \"\"\"Now write the explanation for the following item. Output ONLY the explanation in the same style as the examples:\n",
    "start with \"Entity assignment: ...\" (and \"Predicate ids: ...\" if relevant), followed by a first-person explanation of how the SPARQL answers the question. No bullets, no code fences, no extra headings.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "SPARQL:\n",
    "{sparql}\n",
    "\n",
    "Gold triples (if provided):\n",
    "{triples_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "MODEL       = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS  = 500\n",
    "\n",
    "def load_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        return [json.loads(line) for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict) and isinstance(data.get(\"questions\"), list):\n",
    "        return data[\"questions\"]\n",
    "    if isinstance(data, dict):\n",
    "        return [data]\n",
    "    raise ValueError(\"Unrecognized JSON structure\")\n",
    "\n",
    "def to_human_triples(triples: Any) -> str:\n",
    "    lines: List[str] = []\n",
    "    if isinstance(triples, list):\n",
    "        for t in triples:\n",
    "            if isinstance(t, (list, tuple)) and len(t) == 3:\n",
    "                s, p, o = t\n",
    "                lines.append(f\"- <{s}, {p}, {o}>\")\n",
    "            else:\n",
    "                lines.append(f\"- {t}\")\n",
    "    else:\n",
    "        lines.append(str(triples))\n",
    "    return \"\\n\".join(lines) if lines else \"- (none)\"\n",
    "\n",
    "def build_messages(question: str, triples: Any, formatted_query: str) -> List[Dict[str, str]]:\n",
    "    prompt = FEWSHOT_EXAMPLES + \"\\n\\n\" + ITEM_INSTRUCTIONS.format(\n",
    "        question=question.strip(),\n",
    "        triples_str=to_human_triples(triples),\n",
    "        sparql=formatted_query.strip()\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]  # keep ids short-ish\n",
    "\n",
    "def main():\n",
    "    items = load_any(INPUT_PATH)\n",
    "    items = items\n",
    "    out = []\n",
    "    for idx, entry in enumerate(tqdm(items, desc=\"Building batch JSONL\")):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dynamic_pairs = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dynamic_pairs):\n",
    "            question = (dp.get(\"question\") or \"\").strip()\n",
    "            triples  = dp.get(\"triples\", [])\n",
    "            sparql   = (dp.get(\"sparql\") or \"\").strip()\n",
    "            messages = build_messages(question, triples, sparql)\n",
    "\n",
    "            out.append({\n",
    "                \"custom_id\": f\"{sanitize(eid)}__dp{i}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                    \"messages\": messages\n",
    "                }\n",
    "            })\n",
    "\n",
    "    pathlib.Path(BATCH_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(BATCH_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        for obj in out:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Wrote {len(out)} requests to {BATCH_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff43abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-LsnnGsaAYcPVJBAMUsX7Xv\n",
      "Batch ID: batch_68f6b9270ad08190a6b49c7d6b9794e3\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: finalizing\n",
      "Status: completed\n",
      "Saved: /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH_INPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "BATCH_OUTPUT_FILE_PATH = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "\n",
    "def main():\n",
    "    upload = client.files.create(\n",
    "        file=open(BATCH_INPUT_FILE_PATH, \"rb\"),\n",
    "        purpose=\"batch\",\n",
    "    )\n",
    "    print(\"Uploaded file:\", upload.id)\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"job\": \"LcQUAD_COT\"}\n",
    "    )\n",
    "    print(\"Batch ID:\", batch.id)\n",
    "\n",
    "    # Poll\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        print(\"Status:\", b.status)\n",
    "        if b.status in {\"failed\", \"completed\", \"expired\", \"cancelled\"}:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "    if b.status != \"completed\":\n",
    "        print(\"Batch ended with status:\", b.status)\n",
    "        if getattr(b, \"error_file_id\", None):\n",
    "            err_txt = client.files.content(b.error_file_id).text\n",
    "            print(\"Error file content:\\n\", err_txt[:2000])\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_txt = client.files.content(b.output_file_id).text\n",
    "    with open(BATCH_OUTPUT_FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(out_txt)\n",
    "    print(\"Saved:\", BATCH_OUTPUT_FILE_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f486df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted COT for 740/740 dynamic_pairs.\n",
      "Wrote JSON to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot.json\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, re\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "ORIG_INPUT_PATH     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_gold_dynamic_pairs.json\"\n",
    "BATCH_OUTPUT_FILE   = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_output.jsonl\"\n",
    "MERGED_OUTPUT_JSON  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot.json\"\n",
    "\n",
    "def parse_batch_output(path: str) -> Dict[str, str]:\n",
    "    mapping: Dict[str, str] = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            cid = obj.get(\"custom_id\")\n",
    "            resp = obj.get(\"response\") or {}\n",
    "            status_code = resp.get(\"status_code\")\n",
    "            body = resp.get(\"body\") or {}\n",
    "            if status_code == 200:\n",
    "                try:\n",
    "                    content = body[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                except Exception:\n",
    "                    content = json.dumps(body)[:2000]\n",
    "            else:\n",
    "                err = (body.get(\"error\") or {}).get(\"message\") or f\"Non-200 status: {status_code}\"\n",
    "                content = f\"[ERROR] {err}\"\n",
    "            if cid:\n",
    "                mapping[cid] = content\n",
    "    return mapping\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9._:-]\", \"_\", s)\n",
    "    return s[:120]\n",
    "\n",
    "def _find_items_key_in_dict(d: Dict[str, Any]) -> Optional[str]:\n",
    "    candidate_keys = [k for k, v in d.items() if isinstance(v, list) and all(isinstance(x, dict) for x in v)]\n",
    "    for k in candidate_keys:\n",
    "        v = d[k]\n",
    "        if any(isinstance(x, dict) and \"dynamic_pairs\" in x for x in v):\n",
    "            return k\n",
    "    return candidate_keys[0] if candidate_keys else None\n",
    "\n",
    "def load_container(path: str) -> Tuple[Any, List[Dict[str, Any]], Optional[str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        root = json.load(f)\n",
    "\n",
    "    if isinstance(root, list):\n",
    "        return root, root, None\n",
    "\n",
    "    if isinstance(root, dict):\n",
    "        # Common case: {\"questions\": [...]}\n",
    "        if isinstance(root.get(\"questions\"), list):\n",
    "            return root, root[\"questions\"], \"questions\"\n",
    "\n",
    "        k = _find_items_key_in_dict(root)\n",
    "        if k is None:\n",
    "            raise ValueError(\"Could not locate the list of items in the original JSON.\")\n",
    "        return root, root[k], k\n",
    "\n",
    "    raise ValueError(\"Original JSON must be either a list or a dict containing a list of items.\")\n",
    "\n",
    "def main():\n",
    "    root_obj, items_list, items_key = load_container(ORIG_INPUT_PATH)\n",
    "    cid_to_text = parse_batch_output(BATCH_OUTPUT_FILE)\n",
    "\n",
    "    updated = 0\n",
    "    total_pairs = 0\n",
    "\n",
    "    for idx, entry in enumerate(items_list):\n",
    "        eid = entry.get(\"id\") or entry.get(\"qid\") or idx\n",
    "        dps = entry.get(\"dynamic_pairs\", [])\n",
    "        for i, dp in enumerate(dps):\n",
    "            total_pairs += 1\n",
    "            cid = f\"{sanitize(eid)}__dp{i}\"\n",
    "            if cid in cid_to_text:\n",
    "                dp[\"cot\"] = cid_to_text[cid]\n",
    "                updated += 1\n",
    "\n",
    "    pathlib.Path(MERGED_OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(MERGED_OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(root_obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Inserted COT for {updated}/{total_pairs} dynamic_pairs.\")\n",
    "    print(f\"Wrote JSON to {MERGED_OUTPUT_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788edb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Wrote 150 batch lines to /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\n",
      "Preview of first record:\n",
      " {\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Given a specific question, generate the corresponding SPARQL query for DBpedia. Return your answer after <Answer>, in JSON with key \\\"sparql\\\" and the query as its string value.\\n\\nExample 1 INPUT (exactly what you will receive for every task)\\n\\nQuestion:\\nWhat is the timezone in San Pedro de Atacama?\\n\\nExample 1 OUTPUT (your response must follow **this exact shape**)\\n\\n<Answer>\\n{\\\"sparql\\\": \\\"SELECT DISTINCT ?uri WHERE { res:San_Pedro_de_Atacama dbo:timeZone ?uri }\\\"}\\n<Chain-of-Thought>\\nEntity assignment: 'res:San_Pedro_de_Atacama' for San Pedro de Atacama. Predicate id: 'dbo:timeZone' gives the timezone of a place. I want to find the timezone for San Pedro de Atacama. I directly query the resource for San Pedro de Atacama and retrieve the value of its dbo:timeZone property. The result is the timezone(s) associated \n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from typing import List, Dict, Any, Iterable, Union, Tuple\n",
    "\n",
    "NUM_DEMOS = 3\n",
    "\n",
    "input_path  = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot.json\"\n",
    "batch_jsonl_path     = \"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\"\n",
    "MODEL = \"ft:gpt-3.5-turbo-0125:personal::C9J5ld48\"\n",
    "\n",
    "def _escape_json_string(s: str) -> str:\n",
    "    return (\n",
    "        s.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "         .replace('\"', '\\\\\"')\n",
    "         .replace(\"\\n\", \"\\\\n\")\n",
    "         .replace(\"\\r\", \"\\\\r\")\n",
    "    )\n",
    "\n",
    "GENERIC_INSTR = (\n",
    "    'Given a specific question, generate the corresponding SPARQL query for DBpedia. '\n",
    "    'Return your answer after <Answer>, in JSON with key \"sparql\" and the query as its string value.'\n",
    ")\n",
    "\n",
    "def build_system_msg(sample: Dict[str, Any]) -> Dict[str, str]:\n",
    "    demo_list = sample.get(\"dynamic_pairs\") or sample.get(\"dynamic_paris\") or []\n",
    "    if not demo_list:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    blocks = []\n",
    "    for i, demo in enumerate(demo_list[:NUM_DEMOS], start=1):\n",
    "        demo = demo or {}\n",
    "        demo_q: str = str(demo.get(\"question\", \"\")).strip()\n",
    "        demo_sparql: str = str(demo.get(\"sparql\", \"\")).strip()\n",
    "        demo_cot: str = str(demo.get(\"cot\", \"\")).strip()\n",
    "\n",
    "        if not demo_q or not demo_sparql:\n",
    "            continue\n",
    "\n",
    "        demo_answer = (\n",
    "            \"<Answer>\\n\"\n",
    "            f\"{{\\\"sparql\\\": \\\"{_escape_json_string(demo_sparql)}\\\"}}\"\n",
    "        )\n",
    "        if demo_cot:\n",
    "            demo_answer += f\"\\n<Chain-of-Thought>\\n{_escape_json_string(demo_cot)}\"\n",
    "\n",
    "        block = (\n",
    "            f\"Example {i} INPUT (exactly what you will receive for every task)\\n\\n\"\n",
    "            f\"Question:\\n{demo_q}\\n\\n\"\n",
    "            f\"Example {i} OUTPUT (your response must follow **this exact shape**)\\n\\n\"\n",
    "            f\"{demo_answer}\\n\"\n",
    "        )\n",
    "        blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return {\"role\": \"system\", \"content\": GENERIC_INSTR}\n",
    "\n",
    "    header = (\n",
    "        \"Given a specific question, generate the corresponding SPARQL query for DBpedia. \"\n",
    "        \"Return your answer after <Answer>, in JSON with key \\\"sparql\\\" and the query as its string value.\\n\\n\"\n",
    "    )\n",
    "    content = header + \"\\n\".join(blocks)\n",
    "    return {\"role\": \"system\", \"content\": content}\n",
    "\n",
    "def main():\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    jsonl_rows = []\n",
    "    for sample in dataset:\n",
    "        question = sample.get(\"question\", \"\").strip()\n",
    "\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\"\n",
    "        }\n",
    "        system_msg = build_system_msg(sample)\n",
    "        jsonl_rows.append({\"messages\": [system_msg, user_msg]})\n",
    "\n",
    "    count = 0\n",
    "    with open(batch_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for idx, row in enumerate(jsonl_rows):\n",
    "            messages = row[\"messages\"]\n",
    "            batch_row = {\n",
    "                \"custom_id\": f\"example_{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "            fout.write(json.dumps(batch_row) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"[1/1] Wrote {count} batch lines to {batch_jsonl_path}\")\n",
    "    if jsonl_rows:\n",
    "        print(\"Preview of first record:\\n\", json.dumps(jsonl_rows[0], indent=2)[:900])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c833a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file: file-1Vahe2uZrtfT2w1sYMATgn\n",
      "Batch ID: batch_68f6dea012bc8190ad45326c41af28a2\n",
      "Status: validating\n",
      "Status: validating\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: in_progress\n",
      "Status: completed\n",
      "Saved outputs\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "client = OpenAI()\n",
    "\n",
    "upload = client.files.create(\n",
    "    file=open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_input.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "input_file_id = upload.id\n",
    "print(\"Uploaded file:\", input_file_id)\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id     = input_file_id,\n",
    "    endpoint          = \"/v1/chat/completions\",\n",
    "    completion_window = \"24h\",\n",
    "    metadata          = {\"job\": \"QALD test inference\"}\n",
    ")\n",
    "print(\"Batch ID:\", batch.id)\n",
    "\n",
    "while True:\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    print(\"Status:\", batch.status)\n",
    "    if batch.status in {\"failed\", \"completed\"}:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "if batch.status == \"failed\":\n",
    "    print(\"Batch failed! Full batch object:\")\n",
    "    print(batch)\n",
    "    raise SystemExit(1)\n",
    "\n",
    "result_file_id = batch.output_file_id\n",
    "\n",
    "result_response = client.files.content(result_file_id)\n",
    "\n",
    "with open(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result_response.text)\n",
    "\n",
    "print(\"Saved outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3264b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched file written → /home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_plus_gold.json. Total records: 150\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "GOLD_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_top_10_ft_without_triples_zero_shot_plus_dynamic_pairs_cot.json\")\n",
    "PRED_PATH   = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_batch_output.jsonl\")\n",
    "OUTPUT_PATH = Path(\"/home/m2khoda/dual_retriever/evaluations/end_to_end_evalution/qald_results/qald_test_solo_stage_ft_without_triples_zero_shot_plus_dynamic_pairs_cot_plus_gold.json\")\n",
    "\n",
    "ANSWER_RE = re.compile(r'<Answer>\\s*(\\{.*\\})', re.DOTALL)\n",
    "\n",
    "def extract_sparql(content: str) -> str:\n",
    "    m = ANSWER_RE.search(content)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return json.loads(m.group(1)).get(\"sparql\", \"\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"\"\n",
    "\n",
    "with GOLD_PATH.open(encoding=\"utf-8\") as f:\n",
    "    gold_records = json.load(f)\n",
    "\n",
    "pred_lookup = {}\n",
    "with PRED_PATH.open(encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec     = json.loads(line)\n",
    "        cid     = rec[\"custom_id\"]\n",
    "        content = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        pred_lookup[cid] = extract_sparql(content)\n",
    "\n",
    "for idx, rec in enumerate(gold_records):\n",
    "    cid = f\"example_{idx}\"\n",
    "    rec[\"refined_pred_query\"] = pred_lookup.get(cid, \"\")\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gold_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Enriched file written → {OUTPUT_PATH}. Total records: {len(gold_records)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
